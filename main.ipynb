{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Enhancing Image Understanding and Captioning: Project Segment Overview\n",
        "\n",
        "This segment outlines the initial steps towards enhancing image captioning capabilities, focusing on key frame extraction from videos and leveraging advanced AI models for generating and refining captions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\admin\\anaconda3\\envs\\HKT\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "import random\n",
        "from io import BytesIO\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import requests\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet18\n",
        "from torch.nn.functional import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import (\n",
        "    CLIPProcessor, CLIPModel,\n",
        "    VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer, ViTFeatureExtractor,\n",
        "    AutoModelForCausalLM, AutoProcessor, BlipProcessor, BlipForConditionalGeneration\n",
        ")\n",
        "\n",
        "import openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1 Workflow:\n",
        "1. **Frame Extraction**: Extract middle frames from videos to capture the essence of each scene.\n",
        "2. **Caption Generation**: Generate diverse captions for these frames using ViT_GPT-2, BLIP, and Transformer models.\n",
        "3. **Caption Selection**: Filter captions based on relevance and accuracy using the CLIP Score model, selecting the top two.\n",
        "\n",
        "#### References\n",
        "\n",
        "Bianco, S., Celona, L., Donzella, M., & Napoletano, P. (2023). *Improving Image Captioning Descriptiveness by Ranking and LLM-based Fusion*. arXiv.Org. [https://doi.org/10.48550/arxiv.2306.11593](https://doi.org/10.48550/arxiv.2306.11593)\n",
        "Betker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li, L., Ouyang, L., Zhuang, J., Lee, J., Guo, Y., ... Ramesh, A. (2023). Improving Image Generation with Better Captions. OpenAI. https://cdn.openai.com/papers/dall-e-3.pdf\n",
        "\n",
        "#### Note on Implementation Limitations:\n",
        "Due to limited computational resources, we encountered crashes when attempting to fully implement this step. As a result, the captions used in subsequent steps are the original ones provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_frames_from_dict(folder, video_caption_dict, output_base=\"extracted_frames\", limit=-1):\n",
        "    \"\"\"\n",
        "    Extracts middle frames from videos specified in video_caption_dict and saves them.\n",
        "\n",
        "    Args:\n",
        "    - folder (str): Base directory where video files are stored.\n",
        "    - video_caption_dict (dict): Dictionary with video paths as keys and captions as values.\n",
        "    - output_base (str): Base directory for saving extracted frames.\n",
        "    - limit (int): Maximum number of videos to process. If -1, no limit is applied.\n",
        "    \"\"\"\n",
        "    cnt = 0\n",
        "    for video_path, caption in video_caption_dict.items():\n",
        "        if limit != -1 and cnt >= limit:\n",
        "            break\n",
        "\n",
        "        full_path = os.path.join(folder, video_path)\n",
        "        frame = capture_middle_frame(full_path)\n",
        "\n",
        "        if frame is None:\n",
        "            print(f\"Failed to capture middle frame for {video_path}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        output_dir = Path(output_base) / Path(full_path).parent.name\n",
        "        output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        frame_output_path = output_dir / (Path(full_path).stem + '.png')\n",
        "        frame.save(frame_output_path)\n",
        "        cnt += 1\n",
        "    print(f\"Processed {cnt} videos.\")\n",
        "\n",
        "def capture_middle_frame(video_path):\n",
        "    \"\"\"\n",
        "    Captures and returns the middle frame of a video as a PIL Image.\n",
        "\n",
        "    Args:\n",
        "    - video_path (str): Path to the video file.\n",
        "\n",
        "    Returns:\n",
        "    - PIL.Image or None: The middle frame of the video or None if capture failed.\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Failed to open video: {video_path}\")\n",
        "        return None\n",
        "\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, total_frames // 2)\n",
        "    success, frame = cap.read()\n",
        "    cap.release()\n",
        "\n",
        "    if success:\n",
        "        return Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "    else:\n",
        "        print(\"Failed to capture middle frame.\")\n",
        "        return None\n",
        "def load_captions(jsonl_file_path, limits=-1):\n",
        "    \"\"\"\n",
        "    Loads video captions from a .jsonl file.\n",
        "\n",
        "    Args:\n",
        "    - jsonl_file_path (str): Path to the .jsonl file containing captions.\n",
        "    - limits (int): Maximum number of captions to load. If -1, no limit is applied.\n",
        "\n",
        "    Returns:\n",
        "    - dict: A dictionary mapping video paths to captions.\n",
        "    \"\"\"\n",
        "    caption_video_dict = {}\n",
        "    with open(jsonl_file_path, 'r') as file:\n",
        "        for cnt, line in enumerate(file):\n",
        "            if limits != -1 and cnt >= limits:\n",
        "                break\n",
        "            data = json.loads(line)\n",
        "            caption_video_dict[data['clip']] = [data['caption']]\n",
        "    return caption_video_dict\n",
        "\n",
        "def generate_image_pairs(clip_dict):\n",
        "    \"\"\"\n",
        "    Converts video paths in clip_dict to image paths by changing the file extension.\n",
        "\n",
        "    Args:\n",
        "    - clip_dict (dict): Dictionary with video paths as keys and captions as values.\n",
        "\n",
        "    Returns:\n",
        "    - dict: A dictionary with modified paths pointing to images instead of videos.\n",
        "    \"\"\"\n",
        "    return {Path(key).with_suffix('.png').as_posix(): value for key, value in clip_dict.items()}\n",
        "def load_images(base_folder, image_dict, max_images=4000):\n",
        "    \"\"\"\n",
        "    Loads images from a base folder given an image dictionary.\n",
        "    \"\"\"\n",
        "    images = []\n",
        "    image_names_with_parent = []\n",
        "    count = 0\n",
        "    for relative_path in image_dict.keys():\n",
        "        if count >= max_images:\n",
        "            break\n",
        "        image_path = os.path.join(base_folder, relative_path)\n",
        "        if os.path.isfile(image_path):\n",
        "            img = Image.open(image_path).convert('RGB')\n",
        "            images.append(img)\n",
        "            image_names_with_parent.append(f\"{Path(relative_path).parent.name}/{Path(relative_path).name}\")\n",
        "            count += 1\n",
        "    return images, image_names_with_parent\n",
        "\n",
        "def update_captions_dict(captions_dict, new_captions_list, names):\n",
        "    \"\"\"\n",
        "    Updates the captions in captions_dict with new captions from new_captions_list.\n",
        "    \"\"\"\n",
        "    for name, new_caption in zip(names, new_captions_list):\n",
        "        captions_dict[name] = new_caption\n",
        "    return captions_dict\n",
        "\n",
        "def extract_captions(data):\n",
        "    \"\"\"\n",
        "    Extracts and returns a list of captions from the data dictionary.\n",
        "    \"\"\"\n",
        "    return [caption[0] for caption in data.values()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ViT_GPT2(images):\n",
        "    \"\"\"\n",
        "    Generates captions for a list of images using the ViT_GPT2 model.\n",
        "    \"\"\"\n",
        "    model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "    feature_extractor = ViTFeatureExtractor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")  # Correct class name\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    captions = []\n",
        "    for image in images:\n",
        "        pixel_values = feature_extractor(images=image, return_tensors=\"pt\").pixel_values\n",
        "        pixel_values = pixel_values.to(device)\n",
        "\n",
        "        max_length = 16\n",
        "        num_beams = 4\n",
        "        gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n",
        "\n",
        "        output_ids = model.generate(pixel_values, **gen_kwargs)\n",
        "\n",
        "        preds = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "        captions.append(preds.strip())\n",
        "    print(captions)\n",
        "    return captions\n",
        "\n",
        "def GIT(images):\n",
        "    \"\"\"\n",
        "    Generates captions for a list of images using the GIT model.\n",
        "    \"\"\"\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model_id = \"microsoft/git-large-coco\"\n",
        "    processor = AutoProcessor.from_pretrained(model_id)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
        "\n",
        "    captions = []\n",
        "    for im in images:\n",
        "        inputs = processor(images=im, return_tensors=\"pt\").to(device)\n",
        "        generated_ids = model.generate(pixel_values=inputs.pixel_values, num_beams=3, max_length=20, min_length=5)\n",
        "        generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "        captions.append(generated_caption)\n",
        "    print(captions)\n",
        "    return captions\n",
        "\n",
        "def Transformer(images):\n",
        "    \"\"\"\n",
        "    Generates captions for a list of images using the Transformer model.\n",
        "    \"\"\"\n",
        "    model = pipeline('image-to-text')\n",
        "    captions = []\n",
        "    for im in images:\n",
        "        # Image Captioning\n",
        "        generated_text = model(im)[0]['generated_text']\n",
        "        captions.append(generated_text)\n",
        "    print(captions)\n",
        "    return captions\n",
        "\n",
        "def BLIP(images):\n",
        "    \"\"\"\n",
        "    Generates captions for a list of images using the BLIP model.\n",
        "    \"\"\"\n",
        "    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "    captions = []\n",
        "    for im in images:\n",
        "        inputs = processor(text=None, images=im, return_tensors=\"pt\", padding=True)\n",
        "        outputs = model.generate(**inputs, max_length=20, num_beams=3, return_dict_in_generate=True, output_scores=True)\n",
        "        caption = processor.decode(outputs.sequences[0], skip_special_tokens=True)\n",
        "        captions.append(caption)\n",
        "    print(captions)\n",
        "    return captions\n",
        "\n",
        "def CLIP_score(images, captions):\n",
        "    model_name = \"openai/clip-vit-base-patch32\"\n",
        "    processor = CLIPProcessor.from_pretrained(model_name)\n",
        "    model = CLIPModel.from_pretrained(model_name)\n",
        "\n",
        "    text_inputs = processor(text=captions, return_tensors=\"pt\", padding=True)\n",
        "    image_inputs = processor(images=images, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    image_embeddings = model.get_image_features(**{\"pixel_values\": image_inputs.pixel_values})\n",
        "    text_embeddings = model.get_text_features(**{\"input_ids\": text_inputs.input_ids, \"attention_mask\": text_inputs.attention_mask})\n",
        "\n",
        "    image_embeddings = image_embeddings / image_embeddings.norm(dim=1, keepdim=True)\n",
        "    text_embeddings = text_embeddings / text_embeddings.norm(dim=1, keepdim=True)\n",
        "\n",
        "    cosine_similarities = torch.einsum('nd,nd->n', image_embeddings, text_embeddings)\n",
        "    cosine_similarities = cosine_similarities.detach().tolist()\n",
        "    print(cosine_similarities)\n",
        "    return cosine_similarities\n",
        "\n",
        "# Compare and Remove low-quality captions\n",
        "def compare(lists, n_keep=1):\n",
        "    '''\n",
        "    lists: [list1, list2,...]\n",
        "    n_keep: number of captions kept after ranking\n",
        "    return max_values_info: ((max1, list of max1), (max2, list of max2), ...)\n",
        "                            max1 > max2 > ....\n",
        "    '''\n",
        "    max_values_info = []\n",
        "\n",
        "    for index in range(len(lists[0])):\n",
        "        values = [(lst[index], i) for i, lst in enumerate(lists, start=0)]\n",
        "        max_values = sorted(values, key=lambda x: x[0], reverse=True)[:n_keep]\n",
        "        max_values_info.append(max_values)\n",
        "    print(max_values_info)\n",
        "    return max_values_info\n",
        "\n",
        "# Filter captions\n",
        "def first_filter(captions, max_scores):\n",
        "    '''\n",
        "    After generating scores and ranking, this function is for choosing captions based on scores\n",
        "\n",
        "    return: list(list(caption)), at index ith, it corresponds to the image at index ith in images list and names list\n",
        "    '''\n",
        "    res = []\n",
        "    for i, score in enumerate(max_scores):\n",
        "        temp = []\n",
        "        for j in score:\n",
        "            temp.append(captions[j[1]][i])\n",
        "        res.append(temp)\n",
        "    print(res)\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TirbTRVsxaP0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9497\n"
          ]
        }
      ],
      "source": [
        "clip_dict = load_captions('cut_part0.jsonl')\n",
        "image_dict = generate_image_pairs(clip_dict)\n",
        "\n",
        "print(len(image_dict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xoOC52FFyR-4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9497\n"
          ]
        }
      ],
      "source": [
        "# Extract old captions\n",
        "old_captions = extract_captions(image_dict)\n",
        "print(len(old_captions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "extract_frames_from_dict('video_clips', clip_dict)\n",
        "images, names = load_images('extracted_frames', image_dict, max_images = 1500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "captions_GIT = GIT(images)\n",
        "captions_BLIP = BLIP(images)\n",
        "captions_ViT = ViT_GPT2(images)\n",
        "captions_Trans = Transformer(images)\n",
        "captions = [captions_BLIP, captions_ViT, old_captions]\n",
        "\n",
        "# Calculate CLIP Score\n",
        "scores_GIT = CLIP_score(images, captions_GIT)\n",
        "scores_BLIP = CLIP_score(images, captions_BLIP)\n",
        "scores_ViT = CLIP_score(images, captions_ViT)\n",
        "scores_Trans = CLIP_score(images, captions_Trans)\n",
        "scores_Extracted = CLIP_score(images, old_captions)\n",
        "\n",
        "scores = [scores_BLIP, scores_ViT, scores_Trans, scores_GIT, scores_Extracted]\n",
        "\n",
        "\n",
        "# Ranking\n",
        "max_scores = compare(scores)\n",
        "filtered_captions = first_filter(captions, max_scores)\n",
        "\n",
        "res = update_captions_dict(clip_dict, filtered_captions, names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugq57PbNx9d-"
      },
      "outputs": [],
      "source": [
        "def write_to_jsonl_with_captions_list(data):\n",
        "    \"\"\"\n",
        "    Writes the given dictionary to a uniquely named JSONL file, where each line is a JSON object\n",
        "    with 'clip' as the key and a 'captions' key linking to a list of captions.\n",
        "\n",
        "    Parameters:\n",
        "    - data: Dictionary containing the data to write.\n",
        "    \"\"\"\n",
        "    file_path = f'clips_captions.jsonl'\n",
        "\n",
        "    with open(file_path, 'w') as f:\n",
        "        for key, captions in data.items():\n",
        "            # Structure the JSON object to include both the clip and captions\n",
        "            json_object = {\"clip\": key, \"captions\": captions}\n",
        "            # Write the JSON object to the file\n",
        "            f.write(json.dumps(json_object) + '\\n')\n",
        "\n",
        "    return file_path\n",
        "\n",
        "# Call the function with the modified data and get the path to the new file\n",
        "new_captions_file_path = write_to_jsonl_with_captions_list(res)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Object Detection, Caption Enhancement, Image Generation, and Integrated Metric Calculation\n",
        "\n",
        "In this step, we undertake a multifaceted approach to enrich the dataset by generating new images based on enhanced captions and evaluating these images and their captions through a combination of similarity metrics. This process includes several key sub-steps, each contributing to the final goal of creating accurate and relevant generated content.\n",
        "\n",
        "#### Object Detection with YOLOv5\n",
        "- **Model Initialization**: We utilize the YOLOv5 model, pre-trained on a comprehensive dataset, for detecting objects within frames extracted from video clips. This detection step is crucial for identifying the key elements within each scene.\n",
        "- **Frame Extraction and Detection**: For each video, a representative frame is extracted, and distinct objects within this frame are identified. The detected objects provide a contextual foundation for generating more detailed and contextually accurate captions.\n",
        "\n",
        "#### Caption Enhancement with OpenAI's GPT-4\n",
        "- **Enhancement Logic**: Given the list of detected objects and the overall context caption, an enhanced caption is generated using OpenAI's GPT-4. This enhanced caption aims to integrate the detected objects into the context more naturally and descriptively, providing a richer description for image generation.\n",
        "\n",
        "#### Image Generation with DALL-E 3\n",
        "- **Image Creation**: Using the enhanced captions, new images are generated that visually represent the described scene. This image generation step leverages the DALL-E 3 model to produce images that closely match the detailed descriptions provided.\n",
        "\n",
        "#### Captioning Generated Images with BLIP\n",
        "- **Caption Generation**: The BLIP model is employed to generate captions for the newly created images. This automated captioning process provides a textual description of the content of generated images, facilitating a direct comparison between the original and generated content.\n",
        "\n",
        "#### Integrated Metric Calculation and Similarity Scoring\n",
        "- **Evaluation Framework**: The relevance and quality of generated images and captions are evaluated based on image similarity, text (caption) similarity, and CLIP scores. These metrics are integrated into a single metric to assess the accuracy and relevance of each generated output comprehensively.\n",
        "    - **Image Similarity Score**: Quantifies the visual similarity between the generated image and the original frame.\n",
        "    - **Text Similarity Score**: Measures the semantic closeness between the generated and original captions.\n",
        "    - **CLIP Score**: Evaluates the contextual coherence between the generated image and caption.\n",
        "\n",
        "#### References\n",
        "Betker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li, L., Ouyang, L., Zhuang, J., Lee, J., Guo, Y., ... Ramesh, A. (2023). Improving Image Generation with Better Captions. OpenAI. https://cdn.openai.com/papers/dall-e-3.pdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "6Xosr_D_p41R"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\admin/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
            "YOLOv5  2024-3-5 Python-3.10.13 torch-2.2.1+cpu CPU\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        }
      ],
      "source": [
        "model_yolo = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
        "\n",
        "def get_frame_path(video_path):\n",
        "    \"\"\"\n",
        "    Constructs the path for the frame image based on the video's path.\n",
        "    \"\"\"\n",
        "    base_name = os.path.basename(video_path)\n",
        "    subfolder_name, _ = base_name.split('.', 1) \n",
        "    frame_name = base_name.rsplit('.', 1)[0] + \".png\"\n",
        "    frame_path = os.path.join('extracted_frames', subfolder_name, frame_name)\n",
        "\n",
        "    if os.path.exists(frame_path):\n",
        "        return frame_path\n",
        "    else:\n",
        "        print(f\"Frame does not exist: {frame_path}\")\n",
        "        return None\n",
        "\n",
        "def detect_objects_in_frame(video_path):\n",
        "    \"\"\"\n",
        "    Detects objects in a frame and returns a list of unique detected object names.\n",
        "    \"\"\"\n",
        "    frame_path = get_frame_path(video_path) \n",
        "\n",
        "    if frame_path is None:\n",
        "        return []\n",
        "\n",
        "    results = model_yolo(frame_path)\n",
        "    detected_objects = [results.names[int(obj)] for obj in results.xyxy[0][:, -1]]\n",
        "    unique_objects = list(set([str(obj) for obj in detected_objects]))\n",
        "\n",
        "    return unique_objects\n",
        "def enhance_caption_with_openai(object_names ,overall_context_caption):\n",
        "    \"\"\"\n",
        "    Enhances the given caption with detailed scene description using OpenAI's GPT-4.\n",
        "    \"\"\"\n",
        "    integrated_caption = \" \".join(object_names)\n",
        "    prompt = f\"\"\"\n",
        "    Task: You are part of a team of bots that creates images. You work with an assistant bot that will draw anything\n",
        "    you say in square brackets . For example , outputting \" a beautiful morning in the woods with the sun peaking\n",
        "    through the trees \" will trigger your partner bot to output an image of a forest morning , as described .\n",
        "    You will be prompted by people looking to create detailed images. The way to accomplish this\n",
        "    is to take their short prompts(15-80 words) and make them extremely detailed and descriptive. You will be given overall context caption\n",
        "    and list of other components which are in the same image and same context.\n",
        "\n",
        "    Overall Context Caption: {overall_context_caption}\n",
        "    List of Components: {object_names}\n",
        "\n",
        "    Rules to follow:\n",
        "    1. Remove components which are abnormal associations for example ’ elephant under a sea ’.or not relevant by comparing to the overall context caption.\n",
        "    2. Construct a scene that logically includes remained components within the setting described by the overall caption.\n",
        "    3. Ensure the scene is described in a realistic manner.\n",
        "    4. Avoid adding emotional, sound, strong adjectives or exaggerations that could create noise to generated image, remember this is the common context.\n",
        "    5. The integrated scene description should provide a clearer picture of the situation as described in the overall context caption.\n",
        "    6. For Descriptions of Components, just link them together, do not provide any redundant desscription.\n",
        "    7. The main object in overall context caption should be the main object in your description.\n",
        "    8. Avoid providing your own comment on the context.\n",
        "    Integrated Scene Description:\n",
        "    \"\"\"\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4-0125-preview\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an excellent prompt engineering.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens = 100,\n",
        "        seed = 42,\n",
        "        temperature = 0.2\n",
        "\n",
        "    )\n",
        "    enhanced_caption = response.choices[0].message.content.strip()\n",
        "    return enhanced_caption\n",
        "def generate_and_save_image(enhanced_caption, clip_path, folder='generated_images'):\n",
        "    \"\"\"\n",
        "    Generates an image based on the enhanced caption using DALL-E 3 and saves it.\n",
        "    \"\"\"\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "    \n",
        "    base_name = os.path.basename(clip_path)\n",
        "    image_name = base_name.replace('.mp4', '.jpg')  \n",
        "    response = openai.Image.create(\n",
        "        prompt=f\"A image captured by a phone camera in a real-life, common context with specific details: {enhanced_caption}.\",\n",
        "        n=1,\n",
        "        model=\"dall-e-3\",\n",
        "        style='vivid',\n",
        "        quality=\"standard\"\n",
        "    )\n",
        "\n",
        "    image_data = response['data'][0]['url']\n",
        "    image_response = requests.get(image_data)\n",
        "    image = Image.open(BytesIO(image_response.content))\n",
        "\n",
        "    generated_image_path = os.path.join(folder, image_name) \n",
        "    print(f\"Attempting to save image to {generated_image_path}\")\n",
        "    image.save(generated_image_path)  \n",
        "    return generated_image_path\n",
        "\n",
        "def caption_image_with_blip(image_path):\n",
        "    \"\"\"\n",
        "    Generates a caption for the given image using BLIP model.\n",
        "    \"\"\"\n",
        "    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "    image = Image.open(image_path)\n",
        "    inputs = processor(images=image, return_tensors=\"pt\", padding=True)\n",
        "    outputs = model.generate(**inputs, max_length=20, num_beams=3, return_dict_in_generate=True, output_scores=True)\n",
        "\n",
        "    caption = processor.decode(outputs.sequences[0], skip_special_tokens=True)\n",
        "\n",
        "    return caption\n",
        "\n",
        "def save_generated_image_info(json_file_path, clip_path, generated_image_path, generated_caption, enhanced_caption, original_caption):\n",
        "    \"\"\"\n",
        "    Saves information about the generated image and captions to a JSON file.\n",
        "    \"\"\"\n",
        "    base_name = os.path.basename(json_file_path)\n",
        "    output_json_file_name = base_name.rsplit('.', 1)[0] + \"_generated_info.json\"\n",
        "    output_json_file_path = os.path.join(os.getcwd(), output_json_file_name)\n",
        "\n",
        "    data_to_append = {\n",
        "        \"clip\": clip_path,\n",
        "        \"original_caption\": original_caption,\n",
        "        \"generated_image_path\": generated_image_path,\n",
        "        \"generated_caption\": generated_caption,\n",
        "        \"enhanced_caption\": enhanced_caption\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        if os.path.isfile(output_json_file_path):\n",
        "            with open(output_json_file_path, 'r') as file:\n",
        "                existing_data = json.load(file)\n",
        "        else:\n",
        "            existing_data = []\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading from {output_json_file_path}: {e}\")\n",
        "        existing_data = []\n",
        "\n",
        "    existing_data.append(data_to_append)\n",
        "    with open(output_json_file_path, 'w') as file:\n",
        "        json.dump(existing_data, file, indent=4)\n",
        "\n",
        "def second_filter(json_file_path, images_limit=None):\n",
        "    \"\"\"\n",
        "    Process each video clip to detect objects, enhance captions, generate and caption new images,\n",
        "    and save the information. Limited by an optional images limit.\n",
        "    \"\"\"\n",
        "    openai.api_key = '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'\n",
        "    processed_images_count = 0  \n",
        "\n",
        "    with open(json_file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "        data = [json.loads(line) for line in lines]\n",
        "\n",
        "    for item in data:\n",
        "        if images_limit is not None and processed_images_count >= images_limit:\n",
        "            break  \n",
        "\n",
        "        clip_path = item['clip']\n",
        "        overall_context_caption = item['caption']\n",
        "        video_path = os.path.join('video_clips', clip_path)\n",
        "\n",
        "        try:\n",
        "            detected_objects = detect_objects_in_frame(video_path)\n",
        "            if detected_objects:\n",
        "                objects_names_str = \", \".join(detected_objects)\n",
        "                enhanced_caption = enhance_caption_with_openai(detected_objects, overall_context_caption) \n",
        "                generated_image_path = generate_and_save_image(enhanced_caption, clip_path)\n",
        "                generated_caption = caption_image_with_blip(generated_image_path)\n",
        "                \n",
        "                save_generated_image_info(\n",
        "                    json_file_path,\n",
        "                    item['clip'],\n",
        "                    generated_image_path,\n",
        "                    generated_caption,\n",
        "                    enhanced_caption,\n",
        "                    item['caption']\n",
        "                )\n",
        "\n",
        "                processed_images_count += 1  \n",
        "\n",
        "                print(f\"Processed {item['clip']}: Information saved.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {item['clip']}: {e}\")\n",
        "\n",
        "\n",
        "json_file_path = 'cut_part0.jsonl'\n",
        "images_limit = 0\n",
        "second_filter(json_file_path, images_limit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "jiYZw0dvZxUo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-bmS0RumV9U/-bmS0RumV9U.21_0: Image Similarity Score = 0.714, Text Similarity Score = 0.859, CLIP Score = 0.332, Integrated Metric Score = 1.333\n",
            "-bmS0RumV9U/-bmS0RumV9U.27_0: Image Similarity Score = 0.506, Text Similarity Score = 0.716, CLIP Score = 0.246, Integrated Metric Score = 1.045\n",
            "-bmS0RumV9U/-bmS0RumV9U.27_1: Image Similarity Score = 0.577, Text Similarity Score = 0.479, CLIP Score = 0.353, Integrated Metric Score = 0.982\n",
            "-bmS0RumV9U/-bmS0RumV9U.5_0: Image Similarity Score = 0.516, Text Similarity Score = 0.453, CLIP Score = 0.276, Integrated Metric Score = 0.859\n",
            "-bmS0RumV9U/-bmS0RumV9U.22_0: Image Similarity Score = 0.565, Text Similarity Score = 0.566, CLIP Score = 0.328, Integrated Metric Score = 1.025\n",
            "-bmS0RumV9U/-bmS0RumV9U.25_0: Image Similarity Score = 0.624, Text Similarity Score = 0.387, CLIP Score = 0.242, Integrated Metric Score = 0.814\n",
            "-bmS0RumV9U/-bmS0RumV9U.11_0: Image Similarity Score = 0.627, Text Similarity Score = 0.548, CLIP Score = 0.306, Integrated Metric Score = 1.013\n",
            "-bmS0RumV9U/-bmS0RumV9U.8_0: Image Similarity Score = 0.468, Text Similarity Score = 0.552, CLIP Score = 0.281, Integrated Metric Score = 0.928\n",
            "-bmS0RumV9U/-bmS0RumV9U.26_0: Image Similarity Score = 0.591, Text Similarity Score = 0.852, CLIP Score = 0.274, Integrated Metric Score = 1.221\n",
            "-bmS0RumV9U/-bmS0RumV9U.26_1: Image Similarity Score = 0.627, Text Similarity Score = 0.316, CLIP Score = 0.310, Integrated Metric Score = 0.824\n",
            "-bmS0RumV9U/-bmS0RumV9U.3_0: Image Similarity Score = 0.654, Text Similarity Score = 0.595, CLIP Score = 0.305, Integrated Metric Score = 1.063\n",
            "-bmS0RumV9U/-bmS0RumV9U.30_0: Image Similarity Score = 0.378, Text Similarity Score = 0.708, CLIP Score = 0.304, Integrated Metric Score = 1.046\n",
            "-bmS0RumV9U/-bmS0RumV9U.18_0: Image Similarity Score = 0.753, Text Similarity Score = 0.332, CLIP Score = 0.284, Integrated Metric Score = 0.862\n",
            "-bmS0RumV9U/-bmS0RumV9U.6_0: Image Similarity Score = 0.525, Text Similarity Score = 0.594, CLIP Score = 0.313, Integrated Metric Score = 1.018\n",
            "-bmS0RumV9U/-bmS0RumV9U.0_0: Image Similarity Score = 0.392, Text Similarity Score = 0.714, CLIP Score = 0.279, Integrated Metric Score = 1.030\n",
            "-bmS0RumV9U/-bmS0RumV9U.7_0: Image Similarity Score = 0.636, Text Similarity Score = 0.775, CLIP Score = 0.276, Integrated Metric Score = 1.177\n",
            "-bmS0RumV9U/-bmS0RumV9U.2_0: Image Similarity Score = 0.677, Text Similarity Score = 0.794, CLIP Score = 0.322, Integrated Metric Score = 1.255\n",
            "-bmS0RumV9U/-bmS0RumV9U.29_0: Image Similarity Score = 0.566, Text Similarity Score = 0.344, CLIP Score = 0.276, Integrated Metric Score = 0.790\n",
            "-bmS0RumV9U/-bmS0RumV9U.10_0: Image Similarity Score = 0.697, Text Similarity Score = 0.516, CLIP Score = 0.263, Integrated Metric Score = 0.972\n",
            "-bmS0RumV9U/-bmS0RumV9U.10_1: Image Similarity Score = 0.445, Text Similarity Score = 0.350, CLIP Score = 0.220, Integrated Metric Score = 0.690\n",
            "-bmS0RumV9U/-bmS0RumV9U.13_0: Image Similarity Score = 0.715, Text Similarity Score = 0.539, CLIP Score = 0.270, Integrated Metric Score = 1.005\n",
            "-bmS0RumV9U/-bmS0RumV9U.23_0: Image Similarity Score = 0.617, Text Similarity Score = 0.280, CLIP Score = 0.242, Integrated Metric Score = 0.722\n",
            "-bmS0RumV9U/-bmS0RumV9U.1_0: Image Similarity Score = 0.622, Text Similarity Score = 0.992, CLIP Score = 0.326, Integrated Metric Score = 1.401\n",
            "-bmS0RumV9U/-bmS0RumV9U.4_0: Image Similarity Score = 0.578, Text Similarity Score = 0.340, CLIP Score = 0.258, Integrated Metric Score = 0.773\n",
            "-bmS0RumV9U/-bmS0RumV9U.16_0: Image Similarity Score = 0.714, Text Similarity Score = 0.763, CLIP Score = 0.288, Integrated Metric Score = 1.209\n",
            "-bmS0RumV9U/-bmS0RumV9U.28_0: Image Similarity Score = 0.640, Text Similarity Score = 0.613, CLIP Score = 0.298, Integrated Metric Score = 1.065\n",
            "-bmS0RumV9U/-bmS0RumV9U.28_1: Image Similarity Score = 0.552, Text Similarity Score = 0.285, CLIP Score = 0.265, Integrated Metric Score = 0.724\n",
            "-bmS0RumV9U/-bmS0RumV9U.28_2: Image Similarity Score = 0.639, Text Similarity Score = 0.611, CLIP Score = 0.273, Integrated Metric Score = 1.038\n",
            "-bmS0RumV9U/-bmS0RumV9U.12_0: Image Similarity Score = 0.244, Text Similarity Score = 0.676, CLIP Score = 0.296, Integrated Metric Score = 0.957\n",
            "-bmS0RumV9U/-bmS0RumV9U.9_0: Image Similarity Score = 0.660, Text Similarity Score = 0.644, CLIP Score = 0.369, Integrated Metric Score = 1.170\n",
            "-bmS0RumV9U/-bmS0RumV9U.14_0: Image Similarity Score = 0.694, Text Similarity Score = 0.175, CLIP Score = 0.319, Integrated Metric Score = 0.743\n",
            "-bmS0RumV9U/-bmS0RumV9U.20_0: Image Similarity Score = 0.517, Text Similarity Score = 0.778, CLIP Score = 0.323, Integrated Metric Score = 1.178\n",
            "-bmS0RumV9U/-bmS0RumV9U.15_0: Image Similarity Score = 0.455, Text Similarity Score = 0.171, CLIP Score = 0.224, Integrated Metric Score = 0.549\n",
            "-bmS0RumV9U/-bmS0RumV9U.17_0: Image Similarity Score = 0.537, Text Similarity Score = 0.499, CLIP Score = 0.250, Integrated Metric Score = 0.881\n",
            "-bmS0RumV9U/-bmS0RumV9U.24_0: Image Similarity Score = 0.683, Text Similarity Score = 0.880, CLIP Score = 0.329, Integrated Metric Score = 1.336\n",
            "-bmS0RumV9U/-bmS0RumV9U.19_0: Image Similarity Score = 0.771, Text Similarity Score = 0.596, CLIP Score = 0.303, Integrated Metric Score = 1.108\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.12_1: Image Similarity Score = 0.517, Text Similarity Score = 0.609, CLIP Score = 0.334, Integrated Metric Score = 1.048\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.12_2: Image Similarity Score = 0.486, Text Similarity Score = 0.536, CLIP Score = 0.236, Integrated Metric Score = 0.877\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.12_3: Image Similarity Score = 0.294, Text Similarity Score = 0.722, CLIP Score = 0.351, Integrated Metric Score = 1.070\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.12_0: Image Similarity Score = 0.457, Text Similarity Score = 0.573, CLIP Score = 0.283, Integrated Metric Score = 0.943\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.18_1: Image Similarity Score = 0.408, Text Similarity Score = 0.758, CLIP Score = 0.304, Integrated Metric Score = 1.099\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.18_2: Image Similarity Score = 0.460, Text Similarity Score = 0.630, CLIP Score = 0.277, Integrated Metric Score = 0.986\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.18_0: Image Similarity Score = 0.557, Text Similarity Score = 0.873, CLIP Score = 0.307, Integrated Metric Score = 1.257\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.4_3: Image Similarity Score = 0.208, Text Similarity Score = 0.812, CLIP Score = 0.301, Integrated Metric Score = 1.061\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.4_0: Image Similarity Score = 0.028, Text Similarity Score = 0.505, CLIP Score = 0.335, Integrated Metric Score = 0.768\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.4_1: Image Similarity Score = 0.403, Text Similarity Score = 0.890, CLIP Score = 0.368, Integrated Metric Score = 1.271\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.6_7: Image Similarity Score = 0.452, Text Similarity Score = 0.702, CLIP Score = 0.307, Integrated Metric Score = 1.073\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.6_1: Image Similarity Score = 0.042, Text Similarity Score = 0.650, CLIP Score = 0.272, Integrated Metric Score = 0.830\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.6_9: Image Similarity Score = 0.303, Text Similarity Score = 0.426, CLIP Score = 0.259, Integrated Metric Score = 0.735\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.6_2: Image Similarity Score = 0.577, Text Similarity Score = 0.874, CLIP Score = 0.278, Integrated Metric Score = 1.237\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.6_3: Image Similarity Score = 0.634, Text Similarity Score = 0.744, CLIP Score = 0.300, Integrated Metric Score = 1.173\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.6_5: Image Similarity Score = 0.481, Text Similarity Score = 0.457, CLIP Score = 0.268, Integrated Metric Score = 0.842\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.0_4: Image Similarity Score = 0.729, Text Similarity Score = 0.475, CLIP Score = 0.301, Integrated Metric Score = 0.989\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.0_5: Image Similarity Score = 0.557, Text Similarity Score = 0.492, CLIP Score = 0.257, Integrated Metric Score = 0.891\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.0_7: Image Similarity Score = 0.487, Text Similarity Score = 0.537, CLIP Score = 0.288, Integrated Metric Score = 0.930\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.0_8: Image Similarity Score = 0.435, Text Similarity Score = 0.237, CLIP Score = 0.265, Integrated Metric Score = 0.637\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.0_9: Image Similarity Score = 0.525, Text Similarity Score = 1.000, CLIP Score = 0.335, Integrated Metric Score = 1.378\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.0_2: Image Similarity Score = 0.494, Text Similarity Score = 1.000, CLIP Score = 0.313, Integrated Metric Score = 1.343\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.0_3: Image Similarity Score = 0.489, Text Similarity Score = 0.483, CLIP Score = 0.298, Integrated Metric Score = 0.896\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.5_0: Image Similarity Score = 0.550, Text Similarity Score = 0.721, CLIP Score = 0.264, Integrated Metric Score = 1.084\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.5_1: Image Similarity Score = 0.383, Text Similarity Score = 0.594, CLIP Score = 0.287, Integrated Metric Score = 0.936\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.5_2: Image Similarity Score = 0.488, Text Similarity Score = 0.930, CLIP Score = 0.253, Integrated Metric Score = 1.223\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.16_0: Image Similarity Score = 0.210, Text Similarity Score = 0.526, CLIP Score = 0.265, Integrated Metric Score = 0.788\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.16_1: Image Similarity Score = -0.063, Text Similarity Score = 0.355, CLIP Score = 0.286, Integrated Metric Score = 0.557\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.16_2: Image Similarity Score = 0.239, Text Similarity Score = 0.686, CLIP Score = 0.267, Integrated Metric Score = 0.933\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.19_0: Image Similarity Score = 0.430, Text Similarity Score = 0.579, CLIP Score = 0.261, Integrated Metric Score = 0.915\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.19_1: Image Similarity Score = 0.412, Text Similarity Score = 0.818, CLIP Score = 0.278, Integrated Metric Score = 1.125\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.19_2: Image Similarity Score = 0.249, Text Similarity Score = 0.569, CLIP Score = 0.337, Integrated Metric Score = 0.910\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.3_6: Image Similarity Score = 0.304, Text Similarity Score = 0.667, CLIP Score = 0.293, Integrated Metric Score = 0.970\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.3_7: Image Similarity Score = 0.532, Text Similarity Score = 0.673, CLIP Score = 0.312, Integrated Metric Score = 1.086\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.3_0: Image Similarity Score = 0.459, Text Similarity Score = 0.629, CLIP Score = 0.312, Integrated Metric Score = 1.020\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.3_1: Image Similarity Score = 0.193, Text Similarity Score = 0.637, CLIP Score = 0.290, Integrated Metric Score = 0.898\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.3_2: Image Similarity Score = 0.264, Text Similarity Score = 0.488, CLIP Score = 0.271, Integrated Metric Score = 0.784\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.1_4: Image Similarity Score = 0.441, Text Similarity Score = 0.593, CLIP Score = 0.274, Integrated Metric Score = 0.945\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.1_1: Image Similarity Score = 0.564, Text Similarity Score = 0.667, CLIP Score = 0.307, Integrated Metric Score = 1.088\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.1_2: Image Similarity Score = 0.379, Text Similarity Score = 0.599, CLIP Score = 0.317, Integrated Metric Score = 0.968\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.1_3: Image Similarity Score = 0.548, Text Similarity Score = 0.377, CLIP Score = 0.323, Integrated Metric Score = 0.857\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.21_1: Image Similarity Score = 0.335, Text Similarity Score = 0.677, CLIP Score = 0.345, Integrated Metric Score = 1.043\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.21_3: Image Similarity Score = 0.467, Text Similarity Score = 0.898, CLIP Score = 0.309, Integrated Metric Score = 1.244\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.21_0: Image Similarity Score = 0.182, Text Similarity Score = 0.519, CLIP Score = 0.326, Integrated Metric Score = 0.831\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.7_4: Image Similarity Score = 0.408, Text Similarity Score = 0.516, CLIP Score = 0.328, Integrated Metric Score = 0.922\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.7_0: Image Similarity Score = 0.374, Text Similarity Score = 0.892, CLIP Score = 0.305, Integrated Metric Score = 1.198\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.7_1: Image Similarity Score = 0.277, Text Similarity Score = 0.591, CLIP Score = 0.283, Integrated Metric Score = 0.887\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.7_2: Image Similarity Score = 0.697, Text Similarity Score = 0.660, CLIP Score = 0.325, Integrated Metric Score = 1.154\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.7_3: Image Similarity Score = 0.314, Text Similarity Score = 0.779, CLIP Score = 0.339, Integrated Metric Score = 1.114\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.20_0: Image Similarity Score = 0.369, Text Similarity Score = 0.537, CLIP Score = 0.312, Integrated Metric Score = 0.908\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.20_2: Image Similarity Score = 0.465, Text Similarity Score = 0.589, CLIP Score = 0.364, Integrated Metric Score = 1.041\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.14_0: Image Similarity Score = 0.405, Text Similarity Score = 0.219, CLIP Score = 0.296, Integrated Metric Score = 0.641\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.10_0: Image Similarity Score = 0.435, Text Similarity Score = 0.658, CLIP Score = 0.289, Integrated Metric Score = 1.011\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.10_1: Image Similarity Score = 0.103, Text Similarity Score = 0.457, CLIP Score = 0.314, Integrated Metric Score = 0.736\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.10_2: Image Similarity Score = 0.046, Text Similarity Score = 0.293, CLIP Score = 0.319, Integrated Metric Score = 0.581\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.10_3: Image Similarity Score = 0.408, Text Similarity Score = 0.633, CLIP Score = 0.304, Integrated Metric Score = 0.995\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.10_4: Image Similarity Score = 0.576, Text Similarity Score = 0.871, CLIP Score = 0.300, Integrated Metric Score = 1.256\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.2_0: Image Similarity Score = 0.114, Text Similarity Score = 0.520, CLIP Score = 0.324, Integrated Metric Score = 0.803\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.2_1: Image Similarity Score = 0.197, Text Similarity Score = 0.239, CLIP Score = 0.343, Integrated Metric Score = 0.621\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.2_2: Image Similarity Score = 0.434, Text Similarity Score = 0.692, CLIP Score = 0.306, Integrated Metric Score = 1.056\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.11_0: Image Similarity Score = 0.135, Text Similarity Score = 0.670, CLIP Score = 0.276, Integrated Metric Score = 0.888\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.11_1: Image Similarity Score = -0.112, Text Similarity Score = 0.296, CLIP Score = 0.289, Integrated Metric Score = 0.491\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.11_2: Image Similarity Score = 0.444, Text Similarity Score = 0.785, CLIP Score = 0.325, Integrated Metric Score = 1.157\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.15_0: Image Similarity Score = 0.503, Text Similarity Score = 0.584, CLIP Score = 0.286, Integrated Metric Score = 0.973\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.15_1: Image Similarity Score = 0.252, Text Similarity Score = 0.566, CLIP Score = 0.345, Integrated Metric Score = 0.917\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.15_2: Image Similarity Score = 0.496, Text Similarity Score = 0.620, CLIP Score = 0.294, Integrated Metric Score = 1.010\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.17_0: Image Similarity Score = 0.290, Text Similarity Score = 0.685, CLIP Score = 0.309, Integrated Metric Score = 0.996\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.17_1: Image Similarity Score = 0.381, Text Similarity Score = 0.639, CLIP Score = 0.299, Integrated Metric Score = 0.984\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.8_0: Image Similarity Score = 0.434, Text Similarity Score = 0.617, CLIP Score = 0.332, Integrated Metric Score = 1.020\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.8_1: Image Similarity Score = 0.327, Text Similarity Score = 0.711, CLIP Score = 0.305, Integrated Metric Score = 1.028\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.8_2: Image Similarity Score = 0.378, Text Similarity Score = 0.374, CLIP Score = 0.286, Integrated Metric Score = 0.748\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.8_3: Image Similarity Score = 0.460, Text Similarity Score = 0.497, CLIP Score = 0.315, Integrated Metric Score = 0.913\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.9_0: Image Similarity Score = 0.495, Text Similarity Score = 0.546, CLIP Score = 0.249, Integrated Metric Score = 0.902\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.9_1: Image Similarity Score = 0.431, Text Similarity Score = 0.834, CLIP Score = 0.335, Integrated Metric Score = 1.202\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.13_0: Image Similarity Score = 0.216, Text Similarity Score = 0.545, CLIP Score = 0.368, Integrated Metric Score = 0.909\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.13_1: Image Similarity Score = 0.402, Text Similarity Score = 0.203, CLIP Score = 0.216, Integrated Metric Score = 0.546\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.13_2: Image Similarity Score = 0.404, Text Similarity Score = 0.234, CLIP Score = 0.305, Integrated Metric Score = 0.661\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.22_0: Image Similarity Score = 0.442, Text Similarity Score = 0.196, CLIP Score = 0.265, Integrated Metric Score = 0.606\n",
            "-2VQd6jddug/-2VQd6jddug.13_0: Image Similarity Score = 0.343, Text Similarity Score = 0.327, CLIP Score = 0.320, Integrated Metric Score = 0.730\n",
            "-2VQd6jddug/-2VQd6jddug.8_2: Image Similarity Score = 0.510, Text Similarity Score = 0.229, CLIP Score = 0.336, Integrated Metric Score = 0.731\n",
            "-2VQd6jddug/-2VQd6jddug.18_0: Image Similarity Score = 0.579, Text Similarity Score = 0.754, CLIP Score = 0.367, Integrated Metric Score = 1.227\n",
            "-2VQd6jddug/-2VQd6jddug.2_0: Image Similarity Score = 0.424, Text Similarity Score = 0.868, CLIP Score = 0.273, Integrated Metric Score = 1.165\n",
            "-2VQd6jddug/-2VQd6jddug.27_0: Image Similarity Score = 0.416, Text Similarity Score = 0.475, CLIP Score = 0.284, Integrated Metric Score = 0.846\n",
            "-2VQd6jddug/-2VQd6jddug.26_0: Image Similarity Score = 0.364, Text Similarity Score = 0.761, CLIP Score = 0.391, Integrated Metric Score = 1.171\n",
            "-2VQd6jddug/-2VQd6jddug.20_0: Image Similarity Score = 0.533, Text Similarity Score = 1.000, CLIP Score = 0.321, Integrated Metric Score = 1.368\n",
            "-2VQd6jddug/-2VQd6jddug.23_0: Image Similarity Score = 0.409, Text Similarity Score = 0.823, CLIP Score = 0.312, Integrated Metric Score = 1.162\n",
            "-2VQd6jddug/-2VQd6jddug.23_1: Image Similarity Score = 0.432, Text Similarity Score = 0.539, CLIP Score = 0.302, Integrated Metric Score = 0.924\n",
            "-2VQd6jddug/-2VQd6jddug.21_0: Image Similarity Score = 0.699, Text Similarity Score = 0.888, CLIP Score = 0.320, Integrated Metric Score = 1.340\n",
            "-2VQd6jddug/-2VQd6jddug.21_1: Image Similarity Score = 0.441, Text Similarity Score = 0.552, CLIP Score = 0.258, Integrated Metric Score = 0.895\n",
            "-2VQd6jddug/-2VQd6jddug.21_3: Image Similarity Score = 0.408, Text Similarity Score = 0.904, CLIP Score = 0.326, Integrated Metric Score = 1.242\n",
            "-2VQd6jddug/-2VQd6jddug.21_5: Image Similarity Score = 0.283, Text Similarity Score = 1.000, CLIP Score = 0.343, Integrated Metric Score = 1.290\n",
            "-2VQd6jddug/-2VQd6jddug.30_0: Image Similarity Score = 0.410, Text Similarity Score = 0.886, CLIP Score = 0.298, Integrated Metric Score = 1.200\n",
            "-2VQd6jddug/-2VQd6jddug.30_2: Image Similarity Score = 0.374, Text Similarity Score = 0.642, CLIP Score = 0.325, Integrated Metric Score = 1.010\n",
            "-2VQd6jddug/-2VQd6jddug.30_4: Image Similarity Score = 0.276, Text Similarity Score = 0.624, CLIP Score = 0.288, Integrated Metric Score = 0.918\n",
            "-2VQd6jddug/-2VQd6jddug.30_5: Image Similarity Score = 0.445, Text Similarity Score = 0.481, CLIP Score = 0.335, Integrated Metric Score = 0.914\n",
            "-2VQd6jddug/-2VQd6jddug.16_0: Image Similarity Score = 0.403, Text Similarity Score = 0.862, CLIP Score = 0.393, Integrated Metric Score = 1.273\n",
            "-2VQd6jddug/-2VQd6jddug.24_0: Image Similarity Score = 0.432, Text Similarity Score = 0.624, CLIP Score = 0.281, Integrated Metric Score = 0.973\n",
            "-2VQd6jddug/-2VQd6jddug.19_0: Image Similarity Score = 0.522, Text Similarity Score = 0.696, CLIP Score = 0.323, Integrated Metric Score = 1.112\n",
            "-2VQd6jddug/-2VQd6jddug.5_0: Image Similarity Score = 0.642, Text Similarity Score = 0.828, CLIP Score = 0.287, Integrated Metric Score = 1.234\n",
            "-2VQd6jddug/-2VQd6jddug.1_0: Image Similarity Score = 0.187, Text Similarity Score = 0.764, CLIP Score = 0.290, Integrated Metric Score = 1.001\n",
            "-2VQd6jddug/-2VQd6jddug.22_2: Image Similarity Score = 0.118, Text Similarity Score = 0.463, CLIP Score = 0.323, Integrated Metric Score = 0.756\n",
            "-2VQd6jddug/-2VQd6jddug.22_3: Image Similarity Score = 0.706, Text Similarity Score = 1.000, CLIP Score = 0.283, Integrated Metric Score = 1.399\n",
            "-2VQd6jddug/-2VQd6jddug.22_4: Image Similarity Score = 0.427, Text Similarity Score = 0.823, CLIP Score = 0.322, Integrated Metric Score = 1.179\n",
            "-2VQd6jddug/-2VQd6jddug.7_0: Image Similarity Score = 0.483, Text Similarity Score = 0.585, CLIP Score = 0.298, Integrated Metric Score = 0.979\n",
            "-2VQd6jddug/-2VQd6jddug.3_0: Image Similarity Score = 0.622, Text Similarity Score = 0.718, CLIP Score = 0.331, Integrated Metric Score = 1.178\n",
            "-2VQd6jddug/-2VQd6jddug.9_0: Image Similarity Score = 0.225, Text Similarity Score = 0.585, CLIP Score = 0.261, Integrated Metric Score = 0.838\n",
            "-2VQd6jddug/-2VQd6jddug.9_2: Image Similarity Score = 0.182, Text Similarity Score = 0.461, CLIP Score = 0.312, Integrated Metric Score = 0.769\n",
            "-2VQd6jddug/-2VQd6jddug.11_0: Image Similarity Score = 0.403, Text Similarity Score = 0.573, CLIP Score = 0.365, Integrated Metric Score = 1.004\n",
            "-2VQd6jddug/-2VQd6jddug.11_1: Image Similarity Score = 0.387, Text Similarity Score = 0.843, CLIP Score = 0.324, Integrated Metric Score = 1.181\n",
            "-2VQd6jddug/-2VQd6jddug.11_2: Image Similarity Score = 0.132, Text Similarity Score = 0.135, CLIP Score = 0.330, Integrated Metric Score = 0.496\n",
            "-2VQd6jddug/-2VQd6jddug.4_0: Image Similarity Score = 0.583, Text Similarity Score = 0.901, CLIP Score = 0.292, Integrated Metric Score = 1.276\n",
            "-2VQd6jddug/-2VQd6jddug.0_0: Image Similarity Score = 0.586, Text Similarity Score = 0.827, CLIP Score = 0.301, Integrated Metric Score = 1.225\n",
            "-2VQd6jddug/-2VQd6jddug.17_0: Image Similarity Score = 0.450, Text Similarity Score = 0.560, CLIP Score = 0.280, Integrated Metric Score = 0.926\n",
            "-2VQd6jddug/-2VQd6jddug.25_0: Image Similarity Score = 0.382, Text Similarity Score = 0.404, CLIP Score = 0.305, Integrated Metric Score = 0.795\n",
            "-1ZVsEJ2uVY/-1ZVsEJ2uVY.10_1: Image Similarity Score = 0.650, Text Similarity Score = 0.764, CLIP Score = 0.340, Integrated Metric Score = 1.237\n",
            "-1ZVsEJ2uVY/-1ZVsEJ2uVY.0_0: Image Similarity Score = 0.539, Text Similarity Score = 0.569, CLIP Score = 0.359, Integrated Metric Score = 1.049\n",
            "-1ZVsEJ2uVY/-1ZVsEJ2uVY.0_2: Image Similarity Score = 0.778, Text Similarity Score = 0.791, CLIP Score = 0.275, Integrated Metric Score = 1.245\n",
            "-1ZVsEJ2uVY/-1ZVsEJ2uVY.5_4: Image Similarity Score = 0.636, Text Similarity Score = 0.634, CLIP Score = 0.289, Integrated Metric Score = 1.071\n",
            "-1ZVsEJ2uVY/-1ZVsEJ2uVY.5_5: Image Similarity Score = 0.666, Text Similarity Score = 0.635, CLIP Score = 0.264, Integrated Metric Score = 1.060\n",
            "-1ZVsEJ2uVY/-1ZVsEJ2uVY.5_6: Image Similarity Score = 0.586, Text Similarity Score = 0.480, CLIP Score = 0.278, Integrated Metric Score = 0.912\n",
            "-1ZVsEJ2uVY/-1ZVsEJ2uVY.5_7: Image Similarity Score = 0.679, Text Similarity Score = 0.515, CLIP Score = 0.304, Integrated Metric Score = 1.004\n",
            "-1ZVsEJ2uVY/-1ZVsEJ2uVY.5_0: Image Similarity Score = 0.613, Text Similarity Score = 0.691, CLIP Score = 0.267, Integrated Metric Score = 1.087\n",
            "-1ZVsEJ2uVY/-1ZVsEJ2uVY.5_8: Image Similarity Score = 0.598, Text Similarity Score = 0.725, CLIP Score = 0.292, Integrated Metric Score = 1.136\n",
            "-1ZVsEJ2uVY/-1ZVsEJ2uVY.5_1: Image Similarity Score = 0.623, Text Similarity Score = 0.035, CLIP Score = 0.281, Integrated Metric Score = 0.560\n",
            "-1ZVsEJ2uVY/-1ZVsEJ2uVY.5_9: Image Similarity Score = 0.603, Text Similarity Score = 0.342, CLIP Score = 0.306, Integrated Metric Score = 0.832\n",
            "-1ZVsEJ2uVY/-1ZVsEJ2uVY.5_2: Image Similarity Score = 0.690, Text Similarity Score = 0.795, CLIP Score = 0.304, Integrated Metric Score = 1.243\n",
            "-1ZVsEJ2uVY/-1ZVsEJ2uVY.5_10: Image Similarity Score = 0.534, Text Similarity Score = 0.856, CLIP Score = 0.322, Integrated Metric Score = 1.249\n",
            "-1ZVsEJ2uVY/-1ZVsEJ2uVY.5_3: Image Similarity Score = 0.544, Text Similarity Score = 0.679, CLIP Score = 0.322, Integrated Metric Score = 1.105\n",
            "-1ZVsEJ2uVY/-1ZVsEJ2uVY.4_5: Image Similarity Score = 0.680, Text Similarity Score = 0.613, CLIP Score = 0.325, Integrated Metric Score = 1.107\n",
            "-1ZVsEJ2uVY/-1ZVsEJ2uVY.4_6: Image Similarity Score = 0.713, Text Similarity Score = 0.856, CLIP Score = 0.310, Integrated Metric Score = 1.308\n",
            "-1ZVsEJ2uVY/-1ZVsEJ2uVY.4_7: Image Similarity Score = 0.596, Text Similarity Score = 0.691, CLIP Score = 0.278, Integrated Metric Score = 1.092\n",
            "-1ZVsEJ2uVY/-1ZVsEJ2uVY.4_0: Image Similarity Score = 0.708, Text Similarity Score = 0.474, CLIP Score = 0.342, Integrated Metric Score = 1.020\n",
            "-1ZVsEJ2uVY/-1ZVsEJ2uVY.4_1: Image Similarity Score = 0.535, Text Similarity Score = 0.603, CLIP Score = 0.314, Integrated Metric Score = 1.031\n",
            "-1ZVsEJ2uVY/-1ZVsEJ2uVY.4_2: Image Similarity Score = 0.702, Text Similarity Score = 0.473, CLIP Score = 0.289, Integrated Metric Score = 0.964\n",
            "-1ZVsEJ2uVY/-1ZVsEJ2uVY.4_3: Image Similarity Score = 0.538, Text Similarity Score = 0.806, CLIP Score = 0.321, Integrated Metric Score = 1.208\n",
            "-1ZVsEJ2uVY/-1ZVsEJ2uVY.4_4: Image Similarity Score = 0.648, Text Similarity Score = 0.693, CLIP Score = 0.323, Integrated Metric Score = 1.159\n",
            "-1ZVsEJ2uVY/-1ZVsEJ2uVY.3_4: Image Similarity Score = 0.397, Text Similarity Score = 0.562, CLIP Score = 0.332, Integrated Metric Score = 0.960\n",
            "-1ZVsEJ2uVY/-1ZVsEJ2uVY.3_5: Image Similarity Score = 0.639, Text Similarity Score = 0.552, CLIP Score = 0.317, Integrated Metric Score = 1.033\n",
            "-1ZVsEJ2uVY/-1ZVsEJ2uVY.3_6: Image Similarity Score = 0.376, Text Similarity Score = 0.434, CLIP Score = 0.374, Integrated Metric Score = 0.886\n",
            "-1ZVsEJ2uVY/-1ZVsEJ2uVY.3_7: Image Similarity Score = 0.582, Text Similarity Score = 1.000, CLIP Score = 0.270, Integrated Metric Score = 1.336\n",
            "-1ZVsEJ2uVY/-1ZVsEJ2uVY.3_0: Image Similarity Score = 0.628, Text Similarity Score = 0.829, CLIP Score = 0.317, Integrated Metric Score = 1.260\n",
            "-1ZVsEJ2uVY/-1ZVsEJ2uVY.3_8: Image Similarity Score = 0.545, Text Similarity Score = 0.425, CLIP Score = 0.265, Integrated Metric Score = 0.837\n",
            "-1ZVsEJ2uVY/-1ZVsEJ2uVY.3_1: Image Similarity Score = 0.600, Text Similarity Score = 0.860, CLIP Score = 0.319, Integrated Metric Score = 1.276\n",
            "-1ZVsEJ2uVY/-1ZVsEJ2uVY.3_9: Image Similarity Score = 0.736, Text Similarity Score = 0.733, CLIP Score = 0.294, Integrated Metric Score = 1.198\n",
            "-1ZVsEJ2uVY/-1ZVsEJ2uVY.3_2: Image Similarity Score = 0.561, Text Similarity Score = 0.554, CLIP Score = 0.293, Integrated Metric Score = 0.979\n",
            "-1ZVsEJ2uVY/-1ZVsEJ2uVY.3_10: Image Similarity Score = 0.515, Text Similarity Score = 0.872, CLIP Score = 0.326, Integrated Metric Score = 1.259\n",
            "-1ZVsEJ2uVY/-1ZVsEJ2uVY.3_3: Image Similarity Score = 0.685, Text Similarity Score = 0.491, CLIP Score = 0.288, Integrated Metric Score = 0.971\n",
            "-1ZVsEJ2uVY/-1ZVsEJ2uVY.6_0: Image Similarity Score = 0.542, Text Similarity Score = 0.649, CLIP Score = 0.326, Integrated Metric Score = 1.084\n",
            "-1ZVsEJ2uVY/-1ZVsEJ2uVY.6_1: Image Similarity Score = 0.614, Text Similarity Score = 0.106, CLIP Score = 0.258, Integrated Metric Score = 0.591\n",
            "-1ZVsEJ2uVY/-1ZVsEJ2uVY.6_2: Image Similarity Score = 0.435, Text Similarity Score = 0.734, CLIP Score = 0.307, Integrated Metric Score = 1.093\n",
            "-1ZVsEJ2uVY/-1ZVsEJ2uVY.1_1: Image Similarity Score = 0.476, Text Similarity Score = 0.297, CLIP Score = 0.280, Integrated Metric Score = 0.718\n",
            "-1ZVsEJ2uVY/-1ZVsEJ2uVY.2_0: Image Similarity Score = 0.515, Text Similarity Score = 0.645, CLIP Score = 0.324, Integrated Metric Score = 1.067\n",
            "-1ZVsEJ2uVY/-1ZVsEJ2uVY.8_0: Image Similarity Score = 0.619, Text Similarity Score = 0.587, CLIP Score = 0.308, Integrated Metric Score = 1.044\n",
            "-1ZVsEJ2uVY/-1ZVsEJ2uVY.7_0: Image Similarity Score = 0.541, Text Similarity Score = 0.770, CLIP Score = 0.292, Integrated Metric Score = 1.150\n",
            "-1ZVsEJ2uVY/-1ZVsEJ2uVY.7_1: Image Similarity Score = 0.627, Text Similarity Score = 0.688, CLIP Score = 0.313, Integrated Metric Score = 1.138\n",
            "-1ZVsEJ2uVY/-1ZVsEJ2uVY.7_2: Image Similarity Score = 0.741, Text Similarity Score = 0.389, CLIP Score = 0.320, Integrated Metric Score = 0.940\n",
            "-1ZVsEJ2uVY/-1ZVsEJ2uVY.9_0: Image Similarity Score = 0.362, Text Similarity Score = 0.242, CLIP Score = 0.310, Integrated Metric Score = 0.657\n",
            "-C4LsDAFm_4/-C4LsDAFm_4.0_0: Image Similarity Score = 0.558, Text Similarity Score = 0.577, CLIP Score = 0.295, Integrated Metric Score = 1.000\n",
            "-C4LsDAFm_4/-C4LsDAFm_4.19_0: Image Similarity Score = 0.589, Text Similarity Score = 0.160, CLIP Score = 0.310, Integrated Metric Score = 0.679\n",
            "-C4LsDAFm_4/-C4LsDAFm_4.19_1: Image Similarity Score = 0.559, Text Similarity Score = 0.203, CLIP Score = 0.294, Integrated Metric Score = 0.687\n",
            "-C4LsDAFm_4/-C4LsDAFm_4.6_0: Image Similarity Score = 0.609, Text Similarity Score = 0.326, CLIP Score = 0.287, Integrated Metric Score = 0.803\n",
            "-C4LsDAFm_4/-C4LsDAFm_4.6_1: Image Similarity Score = 0.688, Text Similarity Score = 0.222, CLIP Score = 0.303, Integrated Metric Score = 0.763\n",
            "-C4LsDAFm_4/-C4LsDAFm_4.6_2: Image Similarity Score = 0.581, Text Similarity Score = 0.074, CLIP Score = 0.296, Integrated Metric Score = 0.590\n",
            "-C4LsDAFm_4/-C4LsDAFm_4.9_0: Image Similarity Score = 0.551, Text Similarity Score = 0.372, CLIP Score = 0.312, Integrated Metric Score = 0.843\n",
            "-C4LsDAFm_4/-C4LsDAFm_4.12_0: Image Similarity Score = 0.594, Text Similarity Score = 0.265, CLIP Score = 0.231, Integrated Metric Score = 0.689\n",
            "-C4LsDAFm_4/-C4LsDAFm_4.11_0: Image Similarity Score = 0.487, Text Similarity Score = 0.177, CLIP Score = 0.232, Integrated Metric Score = 0.575\n",
            "-C4LsDAFm_4/-C4LsDAFm_4.16_0: Image Similarity Score = 0.468, Text Similarity Score = 0.533, CLIP Score = 0.309, Integrated Metric Score = 0.940\n",
            "-C4LsDAFm_4/-C4LsDAFm_4.3_0: Image Similarity Score = 0.665, Text Similarity Score = 0.182, CLIP Score = 0.250, Integrated Metric Score = 0.668\n",
            "-C4LsDAFm_4/-C4LsDAFm_4.10_0: Image Similarity Score = 0.647, Text Similarity Score = 0.282, CLIP Score = 0.227, Integrated Metric Score = 0.721\n",
            "-C4LsDAFm_4/-C4LsDAFm_4.10_1: Image Similarity Score = 0.546, Text Similarity Score = 0.176, CLIP Score = 0.275, Integrated Metric Score = 0.640\n",
            "-C4LsDAFm_4/-C4LsDAFm_4.2_0: Image Similarity Score = 0.494, Text Similarity Score = 0.330, CLIP Score = 0.274, Integrated Metric Score = 0.747\n",
            "-C4LsDAFm_4/-C4LsDAFm_4.7_0: Image Similarity Score = 0.680, Text Similarity Score = 0.074, CLIP Score = 0.254, Integrated Metric Score = 0.587\n",
            "-C4LsDAFm_4/-C4LsDAFm_4.13_0: Image Similarity Score = 0.612, Text Similarity Score = 0.274, CLIP Score = 0.270, Integrated Metric Score = 0.743\n",
            "-C4LsDAFm_4/-C4LsDAFm_4.4_0: Image Similarity Score = 0.631, Text Similarity Score = 0.120, CLIP Score = 0.272, Integrated Metric Score = 0.625\n",
            "-C4LsDAFm_4/-C4LsDAFm_4.1_0: Image Similarity Score = 0.444, Text Similarity Score = 0.326, CLIP Score = 0.311, Integrated Metric Score = 0.760\n",
            "-C4LsDAFm_4/-C4LsDAFm_4.1_1: Image Similarity Score = 0.670, Text Similarity Score = 0.331, CLIP Score = 0.261, Integrated Metric Score = 0.805\n",
            "-C4LsDAFm_4/-C4LsDAFm_4.14_0: Image Similarity Score = 0.663, Text Similarity Score = 0.282, CLIP Score = 0.234, Integrated Metric Score = 0.734\n",
            "-C4LsDAFm_4/-C4LsDAFm_4.18_0: Image Similarity Score = 0.660, Text Similarity Score = 0.393, CLIP Score = 0.264, Integrated Metric Score = 0.856\n",
            "-C4LsDAFm_4/-C4LsDAFm_4.18_1: Image Similarity Score = 0.608, Text Similarity Score = 0.152, CLIP Score = 0.253, Integrated Metric Score = 0.623\n",
            "-C4LsDAFm_4/-C4LsDAFm_4.8_0: Image Similarity Score = 0.645, Text Similarity Score = 0.260, CLIP Score = 0.283, Integrated Metric Score = 0.757\n",
            "-C4LsDAFm_4/-C4LsDAFm_4.17_0: Image Similarity Score = 0.478, Text Similarity Score = 0.244, CLIP Score = 0.323, Integrated Metric Score = 0.718\n",
            "-C4LsDAFm_4/-C4LsDAFm_4.17_1: Image Similarity Score = 0.480, Text Similarity Score = 0.329, CLIP Score = 0.258, Integrated Metric Score = 0.724\n",
            "-C4LsDAFm_4/-C4LsDAFm_4.15_0: Image Similarity Score = 0.611, Text Similarity Score = 0.307, CLIP Score = 0.251, Integrated Metric Score = 0.751\n",
            "-C4LsDAFm_4/-C4LsDAFm_4.15_1: Image Similarity Score = 0.450, Text Similarity Score = 0.223, CLIP Score = 0.288, Integrated Metric Score = 0.654\n",
            "-C4LsDAFm_4/-C4LsDAFm_4.20_0: Image Similarity Score = 0.478, Text Similarity Score = 0.587, CLIP Score = 0.306, Integrated Metric Score = 0.986\n",
            "-C4LsDAFm_4/-C4LsDAFm_4.21_0: Image Similarity Score = 0.361, Text Similarity Score = 0.571, CLIP Score = 0.305, Integrated Metric Score = 0.925\n",
            "-C4LsDAFm_4/-C4LsDAFm_4.5_0: Image Similarity Score = 0.428, Text Similarity Score = 0.035, CLIP Score = 0.283, Integrated Metric Score = 0.483\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.21_2: Image Similarity Score = 0.583, Text Similarity Score = 0.636, CLIP Score = 0.284, Integrated Metric Score = 1.048\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.21_0: Image Similarity Score = 0.503, Text Similarity Score = 0.582, CLIP Score = 0.279, Integrated Metric Score = 0.965\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.21_1: Image Similarity Score = 0.631, Text Similarity Score = 0.569, CLIP Score = 0.243, Integrated Metric Score = 0.970\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.6_1: Image Similarity Score = 0.790, Text Similarity Score = 0.455, CLIP Score = 0.302, Integrated Metric Score = 0.997\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.6_2: Image Similarity Score = 0.461, Text Similarity Score = 0.354, CLIP Score = 0.280, Integrated Metric Score = 0.759\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.6_3: Image Similarity Score = 0.555, Text Similarity Score = 0.903, CLIP Score = 0.307, Integrated Metric Score = 1.281\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.7_2: Image Similarity Score = 0.579, Text Similarity Score = 0.117, CLIP Score = 0.260, Integrated Metric Score = 0.589\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.7_3: Image Similarity Score = 0.290, Text Similarity Score = 0.557, CLIP Score = 0.273, Integrated Metric Score = 0.854\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.7_4: Image Similarity Score = 0.553, Text Similarity Score = 0.423, CLIP Score = 0.297, Integrated Metric Score = 0.870\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.7_1: Image Similarity Score = 0.365, Text Similarity Score = 0.380, CLIP Score = 0.310, Integrated Metric Score = 0.772\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.4_0: Image Similarity Score = 0.643, Text Similarity Score = 0.698, CLIP Score = 0.330, Integrated Metric Score = 1.169\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.4_1: Image Similarity Score = 0.762, Text Similarity Score = 0.992, CLIP Score = 0.315, Integrated Metric Score = 1.446\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.18_1: Image Similarity Score = 0.621, Text Similarity Score = 0.970, CLIP Score = 0.250, Integrated Metric Score = 1.307\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.18_0: Image Similarity Score = 0.563, Text Similarity Score = 0.126, CLIP Score = 0.258, Integrated Metric Score = 0.589\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.5_0: Image Similarity Score = 0.655, Text Similarity Score = 0.370, CLIP Score = 0.265, Integrated Metric Score = 0.836\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.0_0: Image Similarity Score = 0.668, Text Similarity Score = 0.817, CLIP Score = 0.360, Integrated Metric Score = 1.308\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.22_0: Image Similarity Score = 0.626, Text Similarity Score = 0.801, CLIP Score = 0.321, Integrated Metric Score = 1.239\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.22_1: Image Similarity Score = 0.716, Text Similarity Score = 0.741, CLIP Score = 0.372, Integrated Metric Score = 1.277\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.20_0: Image Similarity Score = 0.528, Text Similarity Score = 0.786, CLIP Score = 0.305, Integrated Metric Score = 1.171\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.20_1: Image Similarity Score = 0.692, Text Similarity Score = 0.498, CLIP Score = 0.285, Integrated Metric Score = 0.976\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.8_0: Image Similarity Score = 0.698, Text Similarity Score = 0.581, CLIP Score = 0.338, Integrated Metric Score = 1.101\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.8_1: Image Similarity Score = 0.519, Text Similarity Score = 0.207, CLIP Score = 0.341, Integrated Metric Score = 0.721\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.8_2: Image Similarity Score = 0.622, Text Similarity Score = 0.754, CLIP Score = 0.311, Integrated Metric Score = 1.188\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.8_3: Image Similarity Score = 0.552, Text Similarity Score = 0.163, CLIP Score = 0.303, Integrated Metric Score = 0.660\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.12_0: Image Similarity Score = 0.475, Text Similarity Score = 0.456, CLIP Score = 0.282, Integrated Metric Score = 0.852\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.12_1: Image Similarity Score = 0.577, Text Similarity Score = 0.634, CLIP Score = 0.284, Integrated Metric Score = 1.043\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.12_2: Image Similarity Score = 0.593, Text Similarity Score = 0.675, CLIP Score = 0.312, Integrated Metric Score = 1.112\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.12_3: Image Similarity Score = 0.516, Text Similarity Score = 0.443, CLIP Score = 0.308, Integrated Metric Score = 0.884\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.3_1: Image Similarity Score = 0.670, Text Similarity Score = 0.342, CLIP Score = 0.286, Integrated Metric Score = 0.839\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.3_3: Image Similarity Score = 0.517, Text Similarity Score = 0.702, CLIP Score = 0.314, Integrated Metric Score = 1.106\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.3_4: Image Similarity Score = 0.708, Text Similarity Score = 0.766, CLIP Score = 0.339, Integrated Metric Score = 1.261\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.10_0: Image Similarity Score = 0.658, Text Similarity Score = 0.595, CLIP Score = 0.325, Integrated Metric Score = 1.083\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.10_1: Image Similarity Score = 0.714, Text Similarity Score = 0.749, CLIP Score = 0.274, Integrated Metric Score = 1.183\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.19_0: Image Similarity Score = 0.697, Text Similarity Score = 0.328, CLIP Score = 0.311, Integrated Metric Score = 0.864\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.19_1: Image Similarity Score = 0.521, Text Similarity Score = 0.335, CLIP Score = 0.255, Integrated Metric Score = 0.742\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.19_2: Image Similarity Score = 0.672, Text Similarity Score = 0.806, CLIP Score = 0.249, Integrated Metric Score = 1.190\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.9_0: Image Similarity Score = 0.486, Text Similarity Score = 0.379, CLIP Score = 0.324, Integrated Metric Score = 0.835\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.9_1: Image Similarity Score = 0.714, Text Similarity Score = 0.805, CLIP Score = 0.387, Integrated Metric Score = 1.343\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.23_0: Image Similarity Score = 0.622, Text Similarity Score = 0.649, CLIP Score = 0.295, Integrated Metric Score = 1.085\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.23_1: Image Similarity Score = 0.660, Text Similarity Score = 0.138, CLIP Score = 0.274, Integrated Metric Score = 0.653\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.14_1: Image Similarity Score = 0.560, Text Similarity Score = 0.179, CLIP Score = 0.277, Integrated Metric Score = 0.651\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.24_0: Image Similarity Score = 0.678, Text Similarity Score = 0.604, CLIP Score = 0.288, Integrated Metric Score = 1.062\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.24_1: Image Similarity Score = 0.585, Text Similarity Score = 0.210, CLIP Score = 0.301, Integrated Metric Score = 0.709\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.1_0: Image Similarity Score = 0.703, Text Similarity Score = 0.846, CLIP Score = 0.350, Integrated Metric Score = 1.336\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.17_0: Image Similarity Score = 0.599, Text Similarity Score = 0.806, CLIP Score = 0.322, Integrated Metric Score = 1.233\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.17_1: Image Similarity Score = 0.574, Text Similarity Score = 0.483, CLIP Score = 0.289, Integrated Metric Score = 0.921\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.25_0: Image Similarity Score = 0.609, Text Similarity Score = 0.634, CLIP Score = 0.294, Integrated Metric Score = 1.066\n",
            "-_j_HfDgHHA/-_j_HfDgHHA.27_0: Image Similarity Score = 0.295, Text Similarity Score = 0.277, CLIP Score = 0.302, Integrated Metric Score = 0.651\n",
            "-bmS0RumV9U/-bmS0RumV9U.21_0: Image Similarity Score = 0.714, Text Similarity Score = 0.862, CLIP Score = 0.350, Integrated Metric Score = 1.353\n",
            "-bmS0RumV9U/-bmS0RumV9U.27_0: Image Similarity Score = 0.506, Text Similarity Score = 0.873, CLIP Score = 0.318, Integrated Metric Score = 1.248\n",
            "-bmS0RumV9U/-bmS0RumV9U.27_1: Image Similarity Score = 0.577, Text Similarity Score = 0.559, CLIP Score = 0.358, Integrated Metric Score = 1.055\n",
            "-bmS0RumV9U/-bmS0RumV9U.5_0: Image Similarity Score = 0.516, Text Similarity Score = 0.757, CLIP Score = 0.298, Integrated Metric Score = 1.135\n",
            "-bmS0RumV9U/-bmS0RumV9U.22_0: Image Similarity Score = 0.565, Text Similarity Score = 0.395, CLIP Score = 0.299, Integrated Metric Score = 0.854\n",
            "-bmS0RumV9U/-bmS0RumV9U.25_0: Image Similarity Score = 0.624, Text Similarity Score = 0.810, CLIP Score = 0.293, Integrated Metric Score = 1.217\n",
            "-bmS0RumV9U/-bmS0RumV9U.11_0: Image Similarity Score = 0.627, Text Similarity Score = 0.548, CLIP Score = 0.306, Integrated Metric Score = 1.013\n",
            "-bmS0RumV9U/-bmS0RumV9U.8_0: Image Similarity Score = 0.468, Text Similarity Score = 0.665, CLIP Score = 0.297, Integrated Metric Score = 1.038\n",
            "-bmS0RumV9U/-bmS0RumV9U.26_0: Image Similarity Score = 0.591, Text Similarity Score = 0.633, CLIP Score = 0.250, Integrated Metric Score = 1.014\n",
            "-bmS0RumV9U/-bmS0RumV9U.26_1: Image Similarity Score = 0.627, Text Similarity Score = 0.529, CLIP Score = 0.333, Integrated Metric Score = 1.025\n",
            "-bmS0RumV9U/-bmS0RumV9U.3_0: Image Similarity Score = 0.654, Text Similarity Score = 0.820, CLIP Score = 0.341, Integrated Metric Score = 1.286\n",
            "-bmS0RumV9U/-bmS0RumV9U.30_0: Image Similarity Score = 0.378, Text Similarity Score = 0.123, CLIP Score = 0.179, Integrated Metric Score = 0.432\n",
            "-bmS0RumV9U/-bmS0RumV9U.18_0: Image Similarity Score = 0.753, Text Similarity Score = 0.615, CLIP Score = 0.329, Integrated Metric Score = 1.143\n",
            "-bmS0RumV9U/-bmS0RumV9U.6_0: Image Similarity Score = 0.525, Text Similarity Score = 0.785, CLIP Score = 0.358, Integrated Metric Score = 1.222\n",
            "-bmS0RumV9U/-bmS0RumV9U.0_0: Image Similarity Score = 0.392, Text Similarity Score = 0.722, CLIP Score = 0.287, Integrated Metric Score = 1.045\n",
            "-bmS0RumV9U/-bmS0RumV9U.7_0: Image Similarity Score = 0.636, Text Similarity Score = 1.000, CLIP Score = 0.294, Integrated Metric Score = 1.382\n",
            "-bmS0RumV9U/-bmS0RumV9U.2_0: Image Similarity Score = 0.677, Text Similarity Score = 0.966, CLIP Score = 0.349, Integrated Metric Score = 1.425\n",
            "-bmS0RumV9U/-bmS0RumV9U.29_0: Image Similarity Score = 0.566, Text Similarity Score = 0.480, CLIP Score = 0.280, Integrated Metric Score = 0.906\n",
            "-bmS0RumV9U/-bmS0RumV9U.10_0: Image Similarity Score = 0.697, Text Similarity Score = 0.585, CLIP Score = 0.344, Integrated Metric Score = 1.110\n",
            "-bmS0RumV9U/-bmS0RumV9U.10_1: Image Similarity Score = 0.445, Text Similarity Score = 0.301, CLIP Score = 0.293, Integrated Metric Score = 0.722\n",
            "-bmS0RumV9U/-bmS0RumV9U.13_0: Image Similarity Score = 0.715, Text Similarity Score = 0.454, CLIP Score = 0.285, Integrated Metric Score = 0.949\n",
            "-bmS0RumV9U/-bmS0RumV9U.23_0: Image Similarity Score = 0.617, Text Similarity Score = 0.780, CLIP Score = 0.284, Integrated Metric Score = 1.181\n",
            "-bmS0RumV9U/-bmS0RumV9U.1_0: Image Similarity Score = 0.622, Text Similarity Score = 0.734, CLIP Score = 0.292, Integrated Metric Score = 1.152\n",
            "-bmS0RumV9U/-bmS0RumV9U.4_0: Image Similarity Score = 0.578, Text Similarity Score = 0.625, CLIP Score = 0.325, Integrated Metric Score = 1.077\n",
            "-bmS0RumV9U/-bmS0RumV9U.16_0: Image Similarity Score = 0.714, Text Similarity Score = 0.869, CLIP Score = 0.320, Integrated Metric Score = 1.331\n",
            "-bmS0RumV9U/-bmS0RumV9U.28_0: Image Similarity Score = 0.640, Text Similarity Score = 0.815, CLIP Score = 0.321, Integrated Metric Score = 1.256\n",
            "-bmS0RumV9U/-bmS0RumV9U.28_1: Image Similarity Score = 0.552, Text Similarity Score = 0.650, CLIP Score = 0.284, Integrated Metric Score = 1.046\n",
            "-bmS0RumV9U/-bmS0RumV9U.28_2: Image Similarity Score = 0.639, Text Similarity Score = 0.279, CLIP Score = 0.289, Integrated Metric Score = 0.778\n",
            "-bmS0RumV9U/-bmS0RumV9U.12_0: Image Similarity Score = 0.244, Text Similarity Score = 0.463, CLIP Score = 0.255, Integrated Metric Score = 0.738\n",
            "-bmS0RumV9U/-bmS0RumV9U.9_0: Image Similarity Score = 0.660, Text Similarity Score = 0.600, CLIP Score = 0.349, Integrated Metric Score = 1.113\n",
            "-bmS0RumV9U/-bmS0RumV9U.14_0: Image Similarity Score = 0.694, Text Similarity Score = 0.157, CLIP Score = 0.300, Integrated Metric Score = 0.709\n",
            "-bmS0RumV9U/-bmS0RumV9U.20_0: Image Similarity Score = 0.517, Text Similarity Score = 0.784, CLIP Score = 0.325, Integrated Metric Score = 1.185\n",
            "-bmS0RumV9U/-bmS0RumV9U.15_0: Image Similarity Score = 0.455, Text Similarity Score = 0.373, CLIP Score = 0.262, Integrated Metric Score = 0.754\n",
            "-bmS0RumV9U/-bmS0RumV9U.17_0: Image Similarity Score = 0.537, Text Similarity Score = 0.381, CLIP Score = 0.286, Integrated Metric Score = 0.819\n",
            "-bmS0RumV9U/-bmS0RumV9U.24_0: Image Similarity Score = 0.683, Text Similarity Score = 0.796, CLIP Score = 0.320, Integrated Metric Score = 1.257\n",
            "-bmS0RumV9U/-bmS0RumV9U.19_0: Image Similarity Score = 0.771, Text Similarity Score = 0.641, CLIP Score = 0.308, Integrated Metric Score = 1.150\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.12_1: Image Similarity Score = 0.517, Text Similarity Score = 0.603, CLIP Score = 0.331, Integrated Metric Score = 1.040\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.12_2: Image Similarity Score = 0.486, Text Similarity Score = 0.502, CLIP Score = 0.317, Integrated Metric Score = 0.930\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.12_3: Image Similarity Score = 0.294, Text Similarity Score = 0.584, CLIP Score = 0.343, Integrated Metric Score = 0.947\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.12_0: Image Similarity Score = 0.457, Text Similarity Score = 0.703, CLIP Score = 0.296, Integrated Metric Score = 1.065\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.18_1: Image Similarity Score = 0.408, Text Similarity Score = 0.714, CLIP Score = 0.305, Integrated Metric Score = 1.063\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.18_2: Image Similarity Score = 0.460, Text Similarity Score = 0.549, CLIP Score = 0.352, Integrated Metric Score = 0.994\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.18_0: Image Similarity Score = 0.557, Text Similarity Score = 0.837, CLIP Score = 0.312, Integrated Metric Score = 1.232\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.4_3: Image Similarity Score = 0.208, Text Similarity Score = 0.624, CLIP Score = 0.287, Integrated Metric Score = 0.890\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.4_0: Image Similarity Score = 0.028, Text Similarity Score = 0.505, CLIP Score = 0.335, Integrated Metric Score = 0.768\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.4_1: Image Similarity Score = 0.403, Text Similarity Score = 0.856, CLIP Score = 0.390, Integrated Metric Score = 1.265\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.6_7: Image Similarity Score = 0.452, Text Similarity Score = 0.673, CLIP Score = 0.324, Integrated Metric Score = 1.065\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.6_1: Image Similarity Score = 0.042, Text Similarity Score = 0.391, CLIP Score = 0.364, Integrated Metric Score = 0.707\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.6_9: Image Similarity Score = 0.303, Text Similarity Score = 0.981, CLIP Score = 0.306, Integrated Metric Score = 1.245\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.6_2: Image Similarity Score = 0.577, Text Similarity Score = 0.865, CLIP Score = 0.277, Integrated Metric Score = 1.229\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.6_3: Image Similarity Score = 0.634, Text Similarity Score = 0.496, CLIP Score = 0.287, Integrated Metric Score = 0.954\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.6_5: Image Similarity Score = 0.481, Text Similarity Score = 0.619, CLIP Score = 0.307, Integrated Metric Score = 1.015\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.0_4: Image Similarity Score = 0.729, Text Similarity Score = 0.644, CLIP Score = 0.295, Integrated Metric Score = 1.123\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.0_5: Image Similarity Score = 0.557, Text Similarity Score = 0.594, CLIP Score = 0.279, Integrated Metric Score = 0.996\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.0_7: Image Similarity Score = 0.487, Text Similarity Score = 0.837, CLIP Score = 0.340, Integrated Metric Score = 1.233\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.0_8: Image Similarity Score = 0.435, Text Similarity Score = 0.678, CLIP Score = 0.260, Integrated Metric Score = 0.999\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.0_9: Image Similarity Score = 0.525, Text Similarity Score = 0.854, CLIP Score = 0.354, Integrated Metric Score = 1.275\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.0_2: Image Similarity Score = 0.494, Text Similarity Score = 1.000, CLIP Score = 0.313, Integrated Metric Score = 1.343\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.0_3: Image Similarity Score = 0.489, Text Similarity Score = 0.561, CLIP Score = 0.292, Integrated Metric Score = 0.955\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.5_0: Image Similarity Score = 0.550, Text Similarity Score = 0.658, CLIP Score = 0.287, Integrated Metric Score = 1.056\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.5_1: Image Similarity Score = 0.383, Text Similarity Score = 0.865, CLIP Score = 0.294, Integrated Metric Score = 1.168\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.5_2: Image Similarity Score = 0.488, Text Similarity Score = 0.679, CLIP Score = 0.214, Integrated Metric Score = 0.975\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.16_0: Image Similarity Score = 0.210, Text Similarity Score = 0.568, CLIP Score = 0.314, Integrated Metric Score = 0.871\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.16_1: Image Similarity Score = -0.063, Text Similarity Score = 0.385, CLIP Score = 0.301, Integrated Metric Score = 0.596\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.16_2: Image Similarity Score = 0.239, Text Similarity Score = 0.493, CLIP Score = 0.356, Integrated Metric Score = 0.862\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.19_0: Image Similarity Score = 0.430, Text Similarity Score = 0.961, CLIP Score = 0.297, Integrated Metric Score = 1.270\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.19_1: Image Similarity Score = 0.412, Text Similarity Score = 0.543, CLIP Score = 0.269, Integrated Metric Score = 0.886\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.19_2: Image Similarity Score = 0.249, Text Similarity Score = 0.581, CLIP Score = 0.317, Integrated Metric Score = 0.901\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.3_6: Image Similarity Score = 0.304, Text Similarity Score = 0.590, CLIP Score = 0.306, Integrated Metric Score = 0.919\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.3_7: Image Similarity Score = 0.532, Text Similarity Score = 0.900, CLIP Score = 0.331, Integrated Metric Score = 1.293\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.3_0: Image Similarity Score = 0.459, Text Similarity Score = 0.374, CLIP Score = 0.292, Integrated Metric Score = 0.787\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.3_1: Image Similarity Score = 0.193, Text Similarity Score = 0.574, CLIP Score = 0.298, Integrated Metric Score = 0.853\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.3_2: Image Similarity Score = 0.264, Text Similarity Score = 0.630, CLIP Score = 0.299, Integrated Metric Score = 0.929\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.1_4: Image Similarity Score = 0.441, Text Similarity Score = 0.594, CLIP Score = 0.340, Integrated Metric Score = 1.011\n",
            "-4E-rw3AP_o/-4E-rw3AP_o.1_1: Image Similarity Score = 0.564, Text Similarity Score = 0.674, CLIP Score = 0.303, Integrated Metric Score = 1.091\n"
          ]
        }
      ],
      "source": [
        "resnet_model = resnet18(pretrained=True)\n",
        "resnet_model.eval()\n",
        "def integrated_metric(image_similarity_score, text_similarity_score, clip_score):\n",
        "    \"\"\"\n",
        "    Calculates an integrated metric score based on normalized image, text, and clip scores.\n",
        "    The distribution of weights based on the step that component has been through, lower weight for more number of steps.\n",
        "    \"\"\"\n",
        "    acceptable_scores = {'text': 0.6, 'clip': 0.3, 'image': 0.5}\n",
        "    weights = {'image': 0.2, 'text': 0.5, 'clip': 0.3}\n",
        "\n",
        "    normalized_scores = {\n",
        "        'image': (image_similarity_score / acceptable_scores['image']) * weights['image'],\n",
        "        'text': (text_similarity_score / acceptable_scores['text']) * weights['text'],\n",
        "        'clip': (clip_score / acceptable_scores['clip']) * weights['clip']\n",
        "    }\n",
        "\n",
        "    integrated_score = sum(normalized_scores.values())\n",
        "    return integrated_score\n",
        "def load_image(image_path):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    return transform(image).unsqueeze(0)\n",
        "\n",
        "def image_similarity(model, image1, image2):\n",
        "    \"\"\"\n",
        "    This function calculates image similarity using a pretrained ResNet18 model and cosine similarity.\n",
        "    \"\"\"\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        features1 = model(image1)\n",
        "        features2 = model(image2)\n",
        "    return cosine_similarity(features1, features2).item()\n",
        "\n",
        "def calculate_text_similarity(text1, text2, model):\n",
        "    \"\"\"\n",
        "    This function calculates text similarity using a pretrained SentenceTransformer model cosine similarity.\n",
        "    \"\"\"\n",
        "    embedding1 = model.encode(text1, convert_to_tensor=True)\n",
        "    embedding2 = model.encode(text2, convert_to_tensor=True)\n",
        "\n",
        "    similarity = util.pytorch_cos_sim(embedding1, embedding2).item()\n",
        "    return similarity\n",
        "\n",
        "def CLIP_score(image_path, caption):\n",
        "    model_name = \"openai/clip-vit-base-patch32\"\n",
        "    processor = CLIPProcessor.from_pretrained(model_name)\n",
        "    model = CLIPModel.from_pretrained(model_name)\n",
        "\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    images = [image]  \n",
        "\n",
        "    text_inputs = processor(text=[caption], return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    image_inputs = processor(images=images, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    image_embeddings = model.get_image_features(**image_inputs)\n",
        "    text_embeddings = model.get_text_features(**text_inputs)\n",
        "\n",
        "    image_embeddings = image_embeddings / image_embeddings.norm(dim=1, keepdim=True)\n",
        "    text_embeddings = text_embeddings / text_embeddings.norm(dim=1, keepdim=True)\n",
        "\n",
        "    cosine_similarities = torch.einsum('nd,nd->n', image_embeddings, text_embeddings)\n",
        "    cosine_similarity = cosine_similarities.detach().cpu().numpy()[0]\n",
        "\n",
        "    return cosine_similarity\n",
        "\n",
        "def save_met_criteria_enhanced_caption(json_file_path, output_json_path):\n",
        "    \"\"\"\n",
        "    Main function to process the generated images and captions, evaluating them with integrated metrics.\n",
        "    \"\"\"\n",
        "    resnet_model = resnet18(pretrained=True)\n",
        "    resnet_model.eval()\n",
        "    \n",
        "    text_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "    met_criteria_entries = []\n",
        "\n",
        "    with open(json_file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    for item in data:\n",
        "        clip_base = item['clip'].rsplit('.', 1)[0].split(\"/\")[1]\n",
        "        generated_image_path = os.path.join('generated_images', f\"{clip_base}.jpg\")\n",
        "\n",
        "        original_caption = item['original_caption']\n",
        "        generated_caption = item['generated_caption']\n",
        "        enhanced_caption = item['enhanced_caption']\n",
        "\n",
        "        try:\n",
        "            clip_base = item['clip'].rsplit('.', 1)[0]\n",
        "            image1 = load_image(os.path.join('extracted_frames', f\"{clip_base}.png\"))  \n",
        "            image2 = load_image(generated_image_path)\n",
        "            image_similarity_score = image_similarity(resnet_model, image1, image2)\n",
        "        except FileNotFoundError as e:\n",
        "            print(f\"File not found: {e}\")\n",
        "            continue\n",
        "\n",
        "        text_similarity_score = calculate_text_similarity(original_caption, generated_caption, text_model)\n",
        "        clip_score = CLIP_score(generated_image_path, generated_caption)\n",
        "\n",
        "        metric_score = integrated_metric(image_similarity_score, text_similarity_score, clip_score)\n",
        "\n",
        "        print(f\"{clip_base}: Image Similarity Score = {image_similarity_score:.3f}, Text Similarity Score = {text_similarity_score:.3f}, CLIP Score = {clip_score:.3f}, Integrated Metric Score = {metric_score:.3f}\")\n",
        "\n",
        "        if metric_score >= 1.0: \n",
        "            met_criteria_entries.append({\n",
        "                'clip_base': clip_base,\n",
        "                'enhanced_caption': enhanced_caption,\n",
        "                'metric_score': metric_score\n",
        "            })\n",
        "\n",
        "    with open(output_json_path, 'w') as outfile:\n",
        "        json.dump(met_criteria_entries, outfile, indent=4)\n",
        "\n",
        "json_file_path = \"cut_part0_generated_info.json\"\n",
        "output_json_path = \"met_criteria_entries.json\"\n",
        "save_met_criteria_enhanced_caption(json_file_path, output_json_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Final Evaluation Function Overview\n",
        "\n",
        "The `final_evaluation` function performs a comprehensive evaluation of the generated images before and after applying enhanced captions. It assesses how closely these images, both initially generated and regenerated, match the original images from the dataset. This evaluation helps in understanding the effectiveness of using enhanced captions for image regeneration in improving the visual similarity to the original scenes.\n",
        "\n",
        "#### Key Components of the Function\n",
        "\n",
        "- **Image Regeneration and Similarity Calculation**:\n",
        "  - For each selected entry, two key processes are undertaken:\n",
        "    1. **Regeneration of Images**: Generates new images using enhanced captions derived from the original captions but without specific object details. This step aims to produce images that better capture the overall scene or context described in the original caption.\n",
        "    2. **Similarity Scoring**: Computes and compares the similarity scores between the original images and both the initially generated and the newly regenerated images. This comparison provides insights into the impact of the regeneration process on image quality and relevance.\n",
        "\n",
        "- **Evaluation Metrics**:\n",
        "  - Reports the similarity scores for the original versus initially generated images and the original versus regenerated images. It calculates the average similarity scores to provide a broad measure of the regeneration process's effectiveness.\n",
        "\n",
        "\n",
        "#### Purpose and Outcome\n",
        "\n",
        "The primary goal of the `final_evaluation` function is to measure the impact of regenerating images with enhanced captions on the visual similarity to the original images compare with the captions without using YOLOv5 for object detection. A higher average similarity score of selected captions indicates that the approach is effective in producing images that are more aligned with the original context and visual details. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original caption found for -bmS0RumV9U.21_0: a person cutting up apples on a cutting board\n",
            "Attempting to save image to new_generated_images\\-bmS0RumV9U.21_0.jpg\n",
            "Processed -bmS0RumV9U/-bmS0RumV9U.21_0: Old Similarity = 0.9931387901306152, New Similarity = 0.9938879013061523\n",
            "Original caption found for -bmS0RumV9U.27_0: a person pouring milk into a glass\n",
            "Attempting to save image to new_generated_images\\-bmS0RumV9U.27_0.jpg\n",
            "Processed -bmS0RumV9U/-bmS0RumV9U.27_0: Old Similarity = 0.9932164549827576, New Similarity = 0.9939292073249817\n",
            "Original caption found for -bmS0RumV9U.22_0: a person taking a tray of cupcakes out of the oven\n"
          ]
        }
      ],
      "source": [
        "def final_evaluation(json_file_path, captions_file_path, old_images_folder, new_images_folder, resnet_model, num_images):\n",
        "    \"\"\"\n",
        "    Evaluates the similarity between original and generated images before and after enhancement.\n",
        "    \"\"\"\n",
        "    captions_dict = load_captions(captions_file_path)\n",
        "    os.makedirs(new_images_folder, exist_ok=True)\n",
        "\n",
        "    similarity_scores_old = []\n",
        "    similarity_scores_new = []\n",
        "\n",
        "    with open(json_file_path, 'r') as file:\n",
        "        met_criteria_entries = json.load(file)[:num_images]\n",
        "\n",
        "    for entry in met_criteria_entries:\n",
        "        clip_base = entry['clip_base']\n",
        "        clip_folder, clip_name = clip_base.split('/')[1].split('.', 1)\n",
        "        captions_key = f\"{clip_folder}.{clip_name}\"\n",
        "\n",
        "        if captions_key in captions_dict:\n",
        "            original_caption = captions_dict[captions_key]\n",
        "            print(f\"Original caption found for {captions_key}: {original_caption}\")\n",
        "        else:\n",
        "            print(f\"No original caption found for {captions_key}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        original_image_path = os.path.join('extracted_frames', clip_folder, f\"{clip_folder}.{clip_name}.png\")\n",
        "        old_generated_image_path = os.path.join(old_images_folder, f\"{clip_folder}.{clip_name}.jpg\")\n",
        "        new_generated_image_path = os.path.join(new_images_folder, f\"{clip_folder}.{clip_name}.jpg\")\n",
        "\n",
        "        enhanced_caption = enhance_caption_with_openai([], original_caption)\n",
        "        generate_and_save_image(enhanced_caption, new_generated_image_path, folder = new_images_folder)\n",
        "\n",
        "        try:\n",
        "            original_image = load_image(original_image_path)\n",
        "            old_generated_image = load_image(old_generated_image_path)\n",
        "            new_generated_image = load_image(new_generated_image_path)\n",
        "\n",
        "            old_similarity_score = image_similarity(resnet_model, original_image, old_generated_image)\n",
        "            new_similarity_score = image_similarity(resnet_model, original_image, new_generated_image)\n",
        "\n",
        "            similarity_scores_old.append(old_similarity_score)\n",
        "            similarity_scores_new.append(new_similarity_score)\n",
        "\n",
        "            print(f\"Processed {clip_base}: Old Similarity = {old_similarity_score}, New Similarity = {new_similarity_score}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {clip_base}: {e}\")\n",
        "\n",
        "    average_old_similarity = np.mean(similarity_scores_old) if similarity_scores_old else 0\n",
        "    average_new_similarity = np.mean(similarity_scores_new) if similarity_scores_new else 0\n",
        "    print(f\"Average Old Similarity: {average_old_similarity:.4f}\")\n",
        "    print(f\"Average New Similarity: {average_new_similarity:.4f}\")\n",
        "\n",
        "resnet_model = resnet18(pretrained=True)\n",
        "\n",
        "final_evaluation(\n",
        "    json_file_path=\"met_criteria_entries.json\",\n",
        "    captions_file_path=\"cut_part0.jsonl\",\n",
        "    old_images_folder=\"generated_images\",\n",
        "    new_images_folder=\"new_generated_images\",\n",
        "    resnet_model=resnet_model,\n",
        "    num_images=150\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
